{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing the necessary libraries\nWe will be importing libraries at different stages of the program as well, but these are required to start the project","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.image as mpimg\nimport cv2\nimport pydicom as dicom\nfrom keras.callbacks import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:12:51.869011Z","iopub.execute_input":"2022-09-13T17:12:51.869389Z","iopub.status.idle":"2022-09-13T17:12:59.487039Z","shell.execute_reply.started":"2022-09-13T17:12:51.869316Z","shell.execute_reply":"2022-09-13T17:12:59.484879Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\nIn this section we will be pre-processing the data we have and make it suitable for analysis and model creation","metadata":{}},{"cell_type":"code","source":"# Address where data is stored and this will be usefull while declaring other addresses\ndataset_address = \"../input/rsna-pneumonia-detection-challenge/\"","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:12:59.489908Z","iopub.execute_input":"2022-09-13T17:12:59.491304Z","iopub.status.idle":"2022-09-13T17:12:59.498957Z","shell.execute_reply.started":"2022-09-13T17:12:59.491189Z","shell.execute_reply":"2022-09-13T17:12:59.497162Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Importing training csv file as dataframes\n\n# train_label is a dataframe containing original patientId, coordinate for bounding box and target values defining if the patient have pnemonia or not\ntrain_label = pd.read_csv(dataset_address + \"stage_2_train_labels.csv\")\ntrain_label.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:12:59.500993Z","iopub.execute_input":"2022-09-13T17:12:59.502147Z","iopub.status.idle":"2022-09-13T17:12:59.597056Z","shell.execute_reply.started":"2022-09-13T17:12:59.502047Z","shell.execute_reply":"2022-09-13T17:12:59.594569Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function with_box_img_to_numpy reads the image file, prepares a box around the infected area (using x,y,width,height from table)\n# and saves the numpy array of the image file into patientId. This column will help us train the model for\n# object detection.\n\ndef img_to_numpy(patientId):\n    address = dataset_address + \"stage_2_train_images/\" + patientId + \".dcm\" #Address for the file\n    result = train_label.loc[train_label['patientId'] == patientId] #Record for the corresponding patientId\n    \n    ds = dicom.dcmread(address)\n    cv2.imwrite('tempfile.png',ds.pixel_array)\n    \n    img = cv2.imread(\"tempfile.png\")\n    #resizing the image for easier computation while running the model\n    resized_image = cv2.resize(src = img, dsize = (102,102))\n    \n    return np.array(resized_image)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:12:59.600839Z","iopub.execute_input":"2022-09-13T17:12:59.601282Z","iopub.status.idle":"2022-09-13T17:12:59.611912Z","shell.execute_reply.started":"2022-09-13T17:12:59.601252Z","shell.execute_reply":"2022-09-13T17:12:59.609784Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Applying the function to all patientId in train_label dataframe and saving it as df. We'll use this df for our model\n# Training and testing\ndf = train_label.sample(1000)\ndf.patientId = df.patientId.apply(img_to_numpy)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:12:59.613708Z","iopub.execute_input":"2022-09-13T17:12:59.614088Z","iopub.status.idle":"2022-09-13T17:13:53.353677Z","shell.execute_reply.started":"2022-09-13T17:12:59.614057Z","shell.execute_reply":"2022-09-13T17:13:53.352659Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Since we already have a box around the images, droping x,y,width, and height from dataframe\ndf.drop('x',axis = 1,inplace=True)\ndf.drop('y',axis = 1,inplace=True)\ndf.drop('width',axis = 1,inplace=True)\ndf.drop('height',axis = 1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:13:53.354778Z","iopub.execute_input":"2022-09-13T17:13:53.355039Z","iopub.status.idle":"2022-09-13T17:13:53.365663Z","shell.execute_reply.started":"2022-09-13T17:13:53.355015Z","shell.execute_reply":"2022-09-13T17:13:53.364597Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Showing a sample from df for validation\n# df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:13:53.367216Z","iopub.execute_input":"2022-09-13T17:13:53.367670Z","iopub.status.idle":"2022-09-13T17:13:53.377859Z","shell.execute_reply.started":"2022-09-13T17:13:53.367637Z","shell.execute_reply":"2022-09-13T17:13:53.376760Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"##### Data seperation into features and lables\nWe will create 2 lists, one with features (i.e the numpy array of images) and other with Target values","metadata":{}},{"cell_type":"code","source":"x_train_temp = df.drop('Target',axis=1)\ny_train_temp = df['Target']","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:13:53.379701Z","iopub.execute_input":"2022-09-13T17:13:53.380129Z","iopub.status.idle":"2022-09-13T17:13:53.392043Z","shell.execute_reply.started":"2022-09-13T17:13:53.380091Z","shell.execute_reply":"2022-09-13T17:13:53.391025Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"##### Data Visualization for y_train to understand the distribution of data","metadata":{}},{"cell_type":"code","source":"plt.bar(['Normal','Pnemonic'],[y_train_temp[y_train_temp == 0].count(),y_train_temp[y_train_temp == 1].count()],color=['orange','green'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:13:53.393809Z","iopub.execute_input":"2022-09-13T17:13:53.394210Z","iopub.status.idle":"2022-09-13T17:13:53.526128Z","shell.execute_reply.started":"2022-09-13T17:13:53.394106Z","shell.execute_reply":"2022-09-13T17:13:53.525340Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Converting the dataframe into numpy array for easier access using index\nx_train_temp = np.array(x_train_temp)\n\n# Since our dataset only contains 1 column, we are reshaping it in 1D\nx_train_temp = x_train_temp.reshape(1000)\n\ny_train = np.array(y_train_temp)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:05.253545Z","iopub.execute_input":"2022-09-13T17:14:05.253997Z","iopub.status.idle":"2022-09-13T17:14:05.263040Z","shell.execute_reply.started":"2022-09-13T17:14:05.253966Z","shell.execute_reply":"2022-09-13T17:14:05.260315Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x_train = []\nfor x in x_train_temp:\n    x_train.append(x)\nx_train = np.array(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:07.163959Z","iopub.execute_input":"2022-09-13T17:14:07.164681Z","iopub.status.idle":"2022-09-13T17:14:07.186279Z","shell.execute_reply.started":"2022-09-13T17:14:07.164639Z","shell.execute_reply":"2022-09-13T17:14:07.184371Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Creating a validation set\n\nx_train_model, x_val, y_train_model, y_val = train_test_split(x_train,y_train,test_size = 0.3,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:32.040302Z","iopub.execute_input":"2022-09-13T17:14:32.040808Z","iopub.status.idle":"2022-09-13T17:14:32.058016Z","shell.execute_reply.started":"2022-09-13T17:14:32.040777Z","shell.execute_reply":"2022-09-13T17:14:32.055541Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Generation\nTo avoid the problem of bais or overfitting, we are generating artificial images from our existing data.","metadata":{}},{"cell_type":"code","source":"# This library will allow us to generate a lot of images from our existing dataset\nfrom keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:36.182708Z","iopub.execute_input":"2022-09-13T17:14:36.184739Z","iopub.status.idle":"2022-09-13T17:14:36.191235Z","shell.execute_reply.started":"2022-09-13T17:14:36.184657Z","shell.execute_reply":"2022-09-13T17:14:36.189549Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# data_geneator is a instance of ImageDataGenerator which later used on x_train to modify the images\ndata_generator = ImageDataGenerator(\n    rotation_range=90,\n    width_shift_range=5.0,\n    height_shift_range=5.0,\n    zoom_range=1.5,\n    fill_mode='nearest',\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1.2\n)\n\n# Generating the data from x_train\ndata_generator.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:36.751130Z","iopub.execute_input":"2022-09-13T17:14:36.751620Z","iopub.status.idle":"2022-09-13T17:14:36.852356Z","shell.execute_reply.started":"2022-09-13T17:14:36.751588Z","shell.execute_reply":"2022-09-13T17:14:36.850674Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Importing the required libraries to prepare the model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n\n\n# Creating a model taking input as 1024*1024 image\nmodel = Sequential()\n\n# model.add(Flatten(input_shape=[102,102,3]))\n# model.add(Dense(200,activation='relu'))\n# model.add(Dense(100,activation='relu'))\n# model.add(Dense(10,activation='softmax'))\n\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (102,102,3)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:39.591727Z","iopub.execute_input":"2022-09-13T17:14:39.592691Z","iopub.status.idle":"2022-09-13T17:14:39.931981Z","shell.execute_reply.started":"2022-09-13T17:14:39.592656Z","shell.execute_reply":"2022-09-13T17:14:39.930429Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics='accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:42.488413Z","iopub.execute_input":"2022-09-13T17:14:42.488897Z","iopub.status.idle":"2022-09-13T17:14:42.506132Z","shell.execute_reply.started":"2022-09-13T17:14:42.488865Z","shell.execute_reply":"2022-09-13T17:14:42.504686Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = \"rmsprop\" , \n            loss = 'binary_crossentropy' , \n            metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:44.653671Z","iopub.execute_input":"2022-09-13T17:14:44.654175Z","iopub.status.idle":"2022-09-13T17:14:44.670584Z","shell.execute_reply.started":"2022-09-13T17:14:44.654142Z","shell.execute_reply":"2022-09-13T17:14:44.668814Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience = 2, \n                                            verbose=1,\n                                            factor=0.3, \n                                            min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:52.609901Z","iopub.execute_input":"2022-09-13T17:14:52.610395Z","iopub.status.idle":"2022-09-13T17:14:52.616261Z","shell.execute_reply.started":"2022-09-13T17:14:52.610355Z","shell.execute_reply":"2022-09-13T17:14:52.614892Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.fit(data_generator.flow(x_train_model,y_train_model), batch_size = 32,epochs = 100,validation_data = data_generator.flow(x_val, y_val),callbacks = learning_rate_reduction)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T17:14:55.233520Z","iopub.execute_input":"2022-09-13T17:14:55.233946Z","iopub.status.idle":"2022-09-13T17:31:03.747605Z","shell.execute_reply.started":"2022-09-13T17:14:55.233916Z","shell.execute_reply":"2022-09-13T17:31:03.746933Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}